{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d042cd5-58d0-4725-b2e4-0e4f58c8ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain unstructured openai chromadb Cython tiktoken pypdf lark patool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f3b07-be26-4f57-9cd2-052eb2cc22a7",
   "metadata": {},
   "source": [
    "# Retrieval stage:\n",
    "\n",
    "## 1- we'll load the vector store.\n",
    "## 2- retrive the most relevant splits (of documents).\n",
    "## 3- combine the context (the most relevent splits) with a LangChain prompt.\n",
    "## 4- pass the prompt containing the context to the LLM (GPT 3.5 turbo) to get the answer.\n",
    "\n",
    "### Note:\n",
    "    We added a memory to make the chatbot \"conversational\" which will follow these steps:\n",
    "    1- get the new question from the user, pass it to the LLM with the conversation memory.\n",
    "    2- if it was a follow-up question, it will paraphrase it as a stand a lone question.\n",
    "    3- pass the new paraphrased question to the retriver and continue with following the last steps from step 2.\n",
    "    \n",
    "![Langchian_second_phase.JPG](./images/Langchain_second_phase.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f2ca-bdd2-462f-9322-af70bb8fa470",
   "metadata": {},
   "source": [
    "## Imports & environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4beca4d-a831-497d-9f66-3adfb74bfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings # to get the embeddings for our docs\n",
    "from langchain.vectorstores import Chroma # dealing with the DB which we saved the docs embedding in\n",
    "from langchain.memory import ConversationBufferMemory # memory for paraphrasing the follow-up question\n",
    "from langchain.chains import ConversationalRetrievalChain # chain from LangChain which will use the memory and the saved docs\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3c35c-9256-4a26-ad82-a275e1184ced",
   "metadata": {},
   "source": [
    "### prepare the embedding function & model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de46dcd-ef1a-4b3c-92b1-1e22f83a277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=\"path_to_save_docs_after_vectorizing/chroma/\", embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d5811-6f94-4be7-834b-e0bd5c38e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e345e72-4768-4a27-93a1-35d75878ce75",
   "metadata": {},
   "source": [
    "### Create an instance for the chatbot memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04717b5-7a3b-492f-925c-e9e98503ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff7c24-9120-4826-b30a-6ab2a187bcbe",
   "metadata": {},
   "source": [
    "### putting it all togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d307356-2f6a-4ff1-a4d4-08759c14137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "      llm=llm,\n",
    "      retriever=vectordb.as_retriever(),\n",
    "      memory=memory,\n",
    "      rephrase_question = False,\n",
    "      verbose= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85015c-76a7-4f1f-84d8-7b8903834e59",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "#### we noticed that the chain returns a string for the first message, and a dictionary for the rest of the conversation, we need the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f055a-a921-4068-9120-1e2db4f02ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_results(result):\n",
    "  if type(result) == str:\n",
    "    print(result)\n",
    "  else:\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635028c-6867-489e-aea3-c12afd81cb0c",
   "metadata": {},
   "source": [
    "### Testing the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02db7e-004d-4ea4-a7b3-2b363bd1c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the different classifications for sunspots?\"\n",
    "result = chain.run({\"question\": question,})\n",
    "printing_results(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
